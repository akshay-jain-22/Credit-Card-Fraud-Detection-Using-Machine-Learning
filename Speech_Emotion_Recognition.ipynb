{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshay-jain-22/Credit-Card-Fraud-Detection-Using-Machine-Learning/blob/main/Speech_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech Emotion Recognition Project"
      ],
      "metadata": {
        "id": "Fop0PWDyvNZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <a href=\"https://github.com/MrCuber/Speech-Emotion-Recognition/graphs/contributors/\"> Contributors to this project:</a>\n",
        "\n",
        "<b>\n",
        "<li>Umesh Chandra Sakinala - 21BCE1782\n",
        "<li>Sujan Kumar Sollety - 21BCE5667\n",
        "<li>Harshith Simha Gurram - 21BCE5653\n",
        "<li>Pulipaka Phani Meghana - 21BCE1345\n",
        "<li>Pandithradhyula Soumya - 21BCE1424\n",
        "</b>"
      ],
      "metadata": {
        "id": "Ol7UU_KGzPrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "7hhNZ1PlYZ83",
        "outputId": "56550f57-f1ef-4952-a06a-a56108baab3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "44UUBF6HYbLl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_token = {\"username\":\"umesh109\",\"key\":\"aef50af22b6fe262935d024c7c135ac8\"}\n",
        "import json\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "75HXz4hVYen3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install np_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HqvIsAW8-Cc",
        "outputId": "3d71a881-bdac-4fd9-8386-ff5b2f4108bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: np_utils in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from np_utils) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a0rq1CFcu1FB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kaggle\n",
        "import os\n",
        "import sys\n",
        "import librosa\n",
        "import librosa.display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import Audio\n",
        "import keras\n",
        "import np_utils\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "metadata": {
        "id": "NM6LriAhA-pO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Datasets"
      ],
      "metadata": {
        "id": "ffX1QoXwZ4xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d ejlok1/cremad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqac_vC9ZjlW",
        "outputId": "b1f7e955-70fa-4243-9036-4262c0e58a0c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/ejlok1/cremad\n",
            "License(s): ODC Attribution License (ODC-By)\n",
            "cremad.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio\n",
        "!kaggle datasets download -d ejlok1/surrey-audiovisual-expressed-emotion-savee\n",
        "!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqzRraARZum-",
        "outputId": "a5ae6eb2-776b-4d4e-9768-b73fe6f187ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "ravdess-emotional-speech-audio.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee\n",
            "License(s): copyright-authors\n",
            "surrey-audiovisual-expressed-emotion-savee.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess\n",
            "License(s): Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)\n",
            "toronto-emotional-speech-set-tess.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip the datasets"
      ],
      "metadata": {
        "id": "UoFxHhcmafie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/cremad.zip\n",
        "!unzip /content/ravdess-emotional-speech-audio.zip\n",
        "!unzip /content/surrey-audiovisual-expressed-emotion-savee.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxGd7x3faiDE",
        "outputId": "64fb4cd1-09ce-4862-fa19-ffd67bc79a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/cremad.zip\n",
            "replace AudioWAV/1001_DFA_ANG_XX.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/toronto-emotional-speech-set-tess.zip"
      ],
      "metadata": {
        "id": "v8l_QTGoauT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paths for Datasets"
      ],
      "metadata": {
        "id": "u6V-qKpMZ-eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ravdess = r\"/content/audio_speech_actors_01-24\"\n",
        "Crema = r\"/content/AudioWAV\"\n",
        "Tess = r\"/content/TESS Toronto emotional speech set data\"\n",
        "Savee = r\"/content/ALL\""
      ],
      "metadata": {
        "id": "ZTgN5LP5Z391"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Ravdess DataFrame"
      ],
      "metadata": {
        "id": "hzQ7wds_bYtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ravdess_directory_list = os.listdir(Ravdess)\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "for dir in ravdess_directory_list:\n",
        "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
        "    actor = os.listdir(Ravdess + '/' + dir)\n",
        "    for file in actor:\n",
        "        part = file.split('.')[0]\n",
        "        part = part.split('-')\n",
        "        # third part in each file represents the emotion associated to that file.\n",
        "        file_emotion.append(int(part[2]))\n",
        "        file_path.append(Ravdess +'/'+ dir + '/' + file)\n",
        "\n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "\n",
        "# changing integers to actual emotions.\n",
        "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
        "Ravdess_df.head()"
      ],
      "metadata": {
        "id": "sCMbOgm9bWGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Crema DataFrame"
      ],
      "metadata": {
        "id": "s-TUT5xkbpl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crema_directory_list = os.listdir(Crema)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in crema_directory_list:\n",
        "    # storing file paths\n",
        "    file_path.append(Crema + '\\\\' + file)\n",
        "    # storing file emotions\n",
        "    part=file.split('_')\n",
        "    if part[2] == 'SAD':\n",
        "        file_emotion.append('sad')\n",
        "    elif part[2] == 'ANG':\n",
        "        file_emotion.append('angry')\n",
        "    elif part[2] == 'DIS':\n",
        "        file_emotion.append('disgust')\n",
        "    elif part[2] == 'FEA':\n",
        "        file_emotion.append('fear')\n",
        "    elif part[2] == 'HAP':\n",
        "        file_emotion.append('happy')\n",
        "    elif part[2] == 'NEU':\n",
        "        file_emotion.append('neutral')\n",
        "    else:\n",
        "        file_emotion.append('Unknown')\n",
        "\n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Crema_df.head()"
      ],
      "metadata": {
        "id": "MGeu_2pNbr4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. TESS dataset"
      ],
      "metadata": {
        "id": "vHiWalzbbxCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tess_directory_list = os.listdir(Tess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for dir in tess_directory_list:\n",
        "    directories = os.listdir(Tess +'/'+ dir)\n",
        "    for file in directories:\n",
        "        part = file.split('.')[0]\n",
        "        part = part.split('_')[2]\n",
        "        if part=='ps':\n",
        "            file_emotion.append('surprise')\n",
        "        else:\n",
        "            file_emotion.append(part)\n",
        "        file_path.append(Tess + dir + '/' + file)\n",
        "\n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Tess_df.head()"
      ],
      "metadata": {
        "id": "L_jN2Jl0bwyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. CREMA-D dataset"
      ],
      "metadata": {
        "id": "NGmBWXGfcBNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The audio files in this dataset are named in a way such that the prefix letters describes the emotion classes as the below:\n",
        "\n",
        "<ul>\n",
        "<li> 'a' = Anger\n",
        "<li> 'd' = Disgust\n",
        "<li> 'f' = Fear\n",
        "<li> 'h' = Happiness\n",
        "<li> 'n' = Neutral\n",
        "<li> 'sa' = Sadness\n",
        "<li> 'su' = Surprise\n",
        "</ul>"
      ],
      "metadata": {
        "id": "l5JqjLmicGuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "savee_directory_list = os.listdir(Savee)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in savee_directory_list:\n",
        "    file_path.append(Savee + '\\\\' + file)\n",
        "    part = file.split('_')[1]\n",
        "    ele = part[:-6]\n",
        "    if ele=='a':\n",
        "        file_emotion.append('angry')\n",
        "    elif ele=='d':\n",
        "        file_emotion.append('disgust')\n",
        "    elif ele=='f':\n",
        "        file_emotion.append('fear')\n",
        "    elif ele=='h':\n",
        "        file_emotion.append('happy')\n",
        "    elif ele=='n':\n",
        "        file_emotion.append('neutral')\n",
        "    elif ele=='sa':\n",
        "        file_emotion.append('sad')\n",
        "    else:\n",
        "        file_emotion.append('surprise')\n",
        "\n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Savee_df.head()"
      ],
      "metadata": {
        "id": "FvrdAcyBcEal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating combined dataframe\n",
        "data_path = pd.concat([Ravdess_df], axis = 0)\n",
        "data_path.to_csv(\"data_path.csv\",index=False)\n",
        "data_path"
      ],
      "metadata": {
        "id": "8TCDUT4yeT8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualisation and Exploration"
      ],
      "metadata": {
        "id": "pag9VrLBeYJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's plot the count of each emotions in our dataset."
      ],
      "metadata": {
        "id": "McobI6wPea4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Odg2709CfwV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path['Emotions'] = data_path['Emotions'].astype('category')\n",
        "\n",
        "plt.title('Count of Emotions', size=16)\n",
        "sns.countplot(data=data_path, x='Emotions')  # Specify x parameter\n",
        "plt.ylabel('Count', size=12)\n",
        "plt.xlabel('Emotions', size=12)\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mQKabKxCeWwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot waveplots and spectograms for audio signals\n",
        "<li> Waveplots - Waveplots let us know the loudness of the audio at a given time\n",
        "<li> Spectograms - A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. It's a representation of frequencies changing with respect to time for given audio/music signals."
      ],
      "metadata": {
        "id": "dgYDgJBqjQT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_waveplot(data, sr, e):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n",
        "    librosa.display.waveshow(data, sr=sr)\n",
        "    plt.show()\n",
        "\n",
        "def create_spectrogram(data, sr, e):\n",
        "    # stft function converts the data into short term fourier transform\n",
        "    X = librosa.stft(data)\n",
        "    Xdb = librosa.amplitude_to_db(abs(X))\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n",
        "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
        "    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n",
        "    plt.colorbar()"
      ],
      "metadata": {
        "id": "UwSs4-oljZgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wave Plots for the Emotions"
      ],
      "metadata": {
        "id": "IHDVmhWrk63U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <li>Fear"
      ],
      "metadata": {
        "id": "hVSxIPSyk-FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion='fear'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "id": "6rOC4JEDjbVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <li>Angry"
      ],
      "metadata": {
        "id": "eEFrh-ralDmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion='angry'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "id": "YyOr2iu1lHjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <li>Sad"
      ],
      "metadata": {
        "id": "lta6tFXqlG2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion = 'sad'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "id": "HagD9-_ClRfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <li>Happy"
      ],
      "metadata": {
        "id": "ObIYreQ-lZhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion='happy'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "id": "nWrPPL0dlddy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set."
      ],
      "metadata": {
        "id": "qVpOJA-IleyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<li>To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed\n",
        "<li>The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n",
        "<li>In order to this to work adding the perturbations must conserve the same label as the original training sample.\n",
        "<li>In images data augmention can be performed by shifting the image, zooming, rotating, cropping ...etc"
      ],
      "metadata": {
        "id": "BSi2JcAk9oYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But We need to check which augmentation techinques works better for our dataset"
      ],
      "metadata": {
        "id": "1oiiF21x95RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.8):\n",
        "    return librosa.effects.time_stretch(data, rate=rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n",
        "\n",
        "# taking any example and checking for techniques.\n",
        "path = np.array(data_path.Path)[1]\n",
        "data, sample_rate = librosa.load(path)"
      ],
      "metadata": {
        "id": "zFDLpVM0-C2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Simple Audio"
      ],
      "metadata": {
        "id": "SUJLan3H-Gza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=data, sr=sample_rate)\n",
        "Audio(path)"
      ],
      "metadata": {
        "id": "R9jMXnQ--NZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Noise Injection"
      ],
      "metadata": {
        "id": "eQhCwZHA-UM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = noise(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "vT-s30Q--XYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see noise injection is a very good augmentation technique because of which we can assure our training model is not overfitted"
      ],
      "metadata": {
        "id": "SiKpqQZy-atT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.Stretching"
      ],
      "metadata": {
        "id": "5-5pYpvI-iSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = stretch(data, rate=0.8)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "pae2sQQR-obi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.Shifting"
      ],
      "metadata": {
        "id": "HXGF19PVAPqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = shift(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "rWbncsmQAVaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.Pitch"
      ],
      "metadata": {
        "id": "f1qtWbPzAbhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = pitch(data, sample_rate)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "U8F8nIwMAhDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am employing noise injection, stretching (i.e., changing speed), and pitch modulation as part of the aforementioned augmentation techniques."
      ],
      "metadata": {
        "id": "-glWxeeEBKJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "lMHnrZTIBk2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data):\n",
        "    # ZCR\n",
        "    result = np.array([])\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "    result=np.hstack((result, zcr)) # stacking horizontally\n",
        "\n",
        "    # Chroma_stft\n",
        "    stft = np.abs(librosa.stft(data))\n",
        "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
        "\n",
        "    # MFCC\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
        "\n",
        "    # Root Mean Square Value\n",
        "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "    result = np.hstack((result, rms)) # stacking horizontally\n",
        "\n",
        "    # MelSpectogram\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mel)) # stacking horizontally\n",
        "\n",
        "    return result\n",
        "\n",
        "def get_features(path):\n",
        "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
        "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "\n",
        "    # without augmentation\n",
        "    res1 = extract_features(data)\n",
        "    result = np.array(res1)\n",
        "\n",
        "    # data with noise\n",
        "    noise_data = noise(data)\n",
        "    res2 = extract_features(noise_data)\n",
        "    result = np.vstack((result, res2)) # stacking vertically\n",
        "\n",
        "    # data with stretching and pitching\n",
        "    new_data = stretch(data)\n",
        "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "    res3 = extract_features(data_stretch_pitch)\n",
        "    result = np.vstack((result, res3)) # stacking vertically\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "sT-bMsBqBnyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = [], []\n",
        "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
        "    feature = get_features(path)\n",
        "    for ele in feature:\n",
        "        X.append(ele)\n",
        "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
        "        Y.append(emotion)"
      ],
      "metadata": {
        "id": "QZSvIPdvBxZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(Y), data_path.Path.shape"
      ],
      "metadata": {
        "id": "3xqSf0diB0sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features = pd.DataFrame(X)\n",
        "Features['labels'] = Y\n",
        "Features.to_csv('features.csv', index=False)\n",
        "Features.head()"
      ],
      "metadata": {
        "id": "xkHeXVwbB2R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have applied data augmentation and extracted the features for each audio files and saved them."
      ],
      "metadata": {
        "id": "ubGby_WbHZCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "PW6sliwkHb0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As of now we have extracted the data, now we need to normalize and split our data for training and testing."
      ],
      "metadata": {
        "id": "26FuWq2pHeLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = Features.iloc[: ,:-1].values\n",
        "Y = Features['labels'].values"
      ],
      "metadata": {
        "id": "ucnVPmlPHZri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As this is a multiclass classification problem onehotencoding our Y.\n",
        "encoder = OneHotEncoder()\n",
        "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "JoS4q9S0Hgyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "36fDtuLoHiOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling our data with sklearn's Standard scaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "Ue9yk77dHj9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making our data compatible to model.\n",
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "V4EGRrKFHmuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "Kjy1OtOVHoyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.optimizers import AdamR\n",
        "model=Sequential()\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(units=8, activation='softmax'))\n",
        "model.compile(optimizer = \"adam\", loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qrDwSH01Hqeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
        "history=model.fit(x_train, y_train, batch_size=64, epochs=76, validation_data=(x_test, y_test), callbacks=[rlrp])"
      ],
      "metadata": {
        "id": "DwNpq5NSHvD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(x_train,y_train)*100 , \"%\")\n",
        "\n",
        "# epochs = [i for i in range(100)]\n",
        "# fig , ax = plt.subplots(1,2)\n",
        "# train_acc = history.history['accuracy']\n",
        "# train_loss = history.history['loss']\n",
        "# test_acc = history.history['val_accuracy']\n",
        "# test_loss = history.history['val_loss']\n",
        "\n",
        "# fig.set_size_inches(20,6)\n",
        "# ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
        "# ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
        "# ax[0].set_title('Training & Testing Loss')\n",
        "# ax[0].legend()\n",
        "# ax[0].set_xlabel(\"Epochs\")\n",
        "\n",
        "# ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
        "# ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
        "# ax[1].set_title('Training & Testing Accuracy')\n",
        "# ax[1].legend()\n",
        "# ax[1].set_xlabel(\"Epochs\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "RRwhKPHSHxrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on test data.\n",
        "pred_test = model.predict(x_test)\n",
        "y_pred = encoder.inverse_transform(pred_test).reshape(-1, 1)\n",
        "y_test = encoder.inverse_transform(y_test)"
      ],
      "metadata": {
        "id": "X0dfWaBUPOGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
        "df['Predicted Labels'] = y_pred.flatten()\n",
        "df['Actual Labels'] = y_test.flatten()\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "YqryAKDmPOz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize = (12, 10))\n",
        "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
        "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
        "plt.title('Confusion Matrix', size=20)\n",
        "plt.xlabel('Predicted Labels', size=14)\n",
        "plt.ylabel('Actual Labels', size=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xYktBiL5PTh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "LOA_yWLcPZJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_instances = np.sum(cm.to_numpy())\n",
        "correct_predictions = np.trace(cm)\n",
        "\n",
        "accuracy = correct_predictions / total_instances\n",
        "print(f'Overall Accuracy: {accuracy:.2%}')"
      ],
      "metadata": {
        "id": "Wn9UeggSP78a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see our model is more accurate in predicting surprise, angry emotions and it makes sense also because audio files of these emotions differ to other audio files in a lot of ways like pitch, speed, noise.\n",
        "\n"
      ],
      "metadata": {
        "id": "VLDMndf7PhKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We overall achieved 67.87% accuracy on our test data and its decent but we can improve it more by applying more augmentation techniques and using other feature extraction methods."
      ],
      "metadata": {
        "id": "B1B643GtQxJq"
      }
    }
  ]
}